## Service: `llm-gateway-service`

### üéØ M·ª•c ti√™u

X√¢y d·ª±ng m·ªôt **Python microservice** ƒë√≥ng vai tr√≤ **LLM Gateway**, cung c·∫•p API t∆∞∆°ng th√≠ch OpenAI-style, cho ph√©p:

* Nh·∫≠n `system prompt`
* Nh·∫≠n `chat messages`
* Nh·∫≠n c√°c config nh∆∞ `temperature`, `max_tokens`
* G·ªçi backend LLM:

  * Ollama (local)
  * OpenAI (cloud)
* Tr·∫£ k·∫øt qu·∫£ text thu·∫ßn (kh√¥ng agent, kh√¥ng search)

---

## 1. Tech stack B·∫ÆT BU·ªòC

* Python 3.10+
* FastAPI
* httpx (async)
* pydantic
* uvicorn

**KH√îNG d√πng**:

* LangChain
* Agent framework
* Vector DB
* Tool calling

---

## 2. C·∫•u tr√∫c th∆∞ m·ª•c PH·∫¢I T·∫†O

```text
llm-gateway-service/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat.py
‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ openai.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îî‚îÄ‚îÄ prompt_utils.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## 3. API SPEC (PH·∫¢I GI·ªêNG OPENAI)

### Endpoint

```
POST /v1/chat/completions
```

### Request body

```json
{
  "model": "ollama:qwen2.5",
  "messages": [
    { "role": "system", "content": "You are a helpful assistant." },
    { "role": "user", "content": "T√≥m t·∫Øt th√¥ng tin v·ªÅ Cao T·∫£i" }
  ],
  "temperature": 0.0,
  "max_tokens": 512,
  "stream": false
}
```

---

### Response body (t·ªëi gi·∫£n nh∆∞ng t∆∞∆°ng th√≠ch)

```json
{
  "id": "chatcmpl-001",
  "object": "chat.completion",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Cao T·∫£i ƒë∆∞·ª£c ghi nh·∫≠n l√†..."
      },
      "finish_reason": "stop"
    }
  ]
}
```

---

## 4. Quy ∆∞·ªõc ch·ªçn backend LLM

* N·∫øu `model` b·∫Øt ƒë·∫ßu b·∫±ng:

  * `ollama:` ‚Üí d√πng Ollama
  * `openai:` ‚Üí d√πng OpenAI

V√≠ d·ª•:

* `ollama:qwen2.5`
* `openai:gpt-4.1-mini`

---

## 5. Interface LLM (PH·∫¢I T√ÅCH RI√äNG)

### `app/llm/base.py`

```python
from abc import ABC, abstractmethod

class BaseLLM(ABC):
    @abstractmethod
    async def chat(self, messages, temperature, max_tokens):
        pass
```

---

## 6. Ollama implementation

### `app/llm/ollama.py`

* G·ªçi API:

  ```
  POST http://localhost:11434/api/chat
  ```

* Payload:

  ```json
  {
    "model": "qwen2.5",
    "messages": [...],
    "stream": false,
    "options": {
      "temperature": 0.0
    }
  }
  ```

* Parse:

  ```json
  response.message.content
  ```

---

## 7. OpenAI implementation

### `app/llm/openai.py`

* D√πng OpenAI Python SDK (async)
* D√πng API `responses.create`
* Map `messages` sang input text
* Ch·ªâ c·∫ßn l·∫•y `output_text`

---

## 8. Router logic (QUAN TR·ªåNG)

### `app/api/chat.py`

Lu·ªìng x·ª≠ l√Ω:

1. Parse request
2. T√°ch provider t·ª´ `model`
3. Init LLM t∆∞∆°ng ·ª©ng
4. G·ªçi `llm.chat(...)`
5. Wrap k·∫øt qu·∫£ v·ªÅ OpenAI-style response

---

## 9. Prompt handling

* **KH√îNG** hard-code prompt
* Client g·ª≠i:

  * `system`
  * `user`
  * `assistant` (n·∫øu c√≥)

Service ch·ªâ **forward nguy√™n messages**

---

## 10. Config file

### `app/config.py`

```python
OLLAMA_BASE_URL=http://localhost:11434
OPENAI_API_KEY=sk-xxx
DEFAULT_TEMPERATURE=0.0
```

---

## 11. Y√™u c·∫ßu ch·∫•t l∆∞·ª£ng

* Async ho√†n to√†n
* Kh√¥ng crash n·∫øu model kh√¥ng t·ªìn t·∫°i
* Log r√µ:

  * provider
  * model
  * token config
* Tr·∫£ l·ªói HTTP 400 n·∫øu request sai

---

## 12. Nh·ªØng th·ª© KH√îNG ƒê∆Ø·ª¢C L√ÄM

‚ùå Kh√¥ng g·ªôp Ollama + OpenAI v√†o c√πng file
‚ùå Kh√¥ng vi·∫øt logic task / summary / gia ph·∫£
‚ùå Kh√¥ng parse n·ªôi dung tr·∫£ v·ªÅ th√†nh JSON
‚ùå Kh√¥ng t·ª± th√™m system prompt

---

## 13. Ti√™u ch√≠ ho√†n th√†nh

Service ƒë∆∞·ª£c coi l√† ho√†n th√†nh khi:

* C√≥ th·ªÉ g·ªçi b·∫±ng curl gi·ªëng OpenAI
* ƒê·ªïi `model` l√† ƒë·ªïi backend
* knowledge-search-service c√≥ th·ªÉ g·ªçi th·∫≥ng

---

## 14. G·ª£i √Ω test nhanh

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ollama:qwen2.5",
    "messages": [
      {"role": "system", "content": "You summarize text."},
      {"role": "user", "content": "T√≥m t·∫Øt v·ªÅ Cao T·∫£i"}
    ],
    "temperature": 0
  }'
```

---

## 15. GHI CH√ö CU·ªêI (r·∫•t quan tr·ªçng)

> `llm-gateway-service` **KH√îNG PH·∫¢I AI AGENT**
> N√≥ l√† **infra service**, c√†ng ngu c√†ng t·ªët
> To√†n b·ªô intelligence n·∫±m ·ªü prompt ph√≠a caller

---